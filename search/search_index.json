{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Korthweb - Orthanc on Kubernetes","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Orthanc is an open-source application suite to ingest, store, distribute and display medical images. Osimis releases Orthanc in Docker images. Deployment of Orthanc can be complex. </p> <p>To automate Orthanc deployment, Digi Hunch created Orthweb project, a reference Orthanc deployment on AWS. Further, we started this Korthweb project to host Orthanc on Kubernetes. Korthweb provides reference deployment paradigms towards a modern, cloud-native and extensible medical imaging solution, combining the power of Orthanc and Kubernetes.</p> <p>The Korthweb project comes with three deployment options, with different automation levels and feature sets. The goal is to deploy Orthanc workload on an established Kubernetes cluster. </p> <p>A solid design of Kubernetes cluster sets the foundation for security, reliability and scalability. However, cluster provisioning itself is beyond the focus of this project. In the <code>Infrastructure</code> section, we provide some tools to build a quick Kubernetes cluster for test purposes.</p> <p>The <code>Deployment</code> section discusses the methodologies, the tooling and the three deployment approaches, with a comparison.</p> <p>The <code>Validation</code> section provides guidelines on how to verify the funtionality after each deployment approach, including produce and send images in DICOM.</p>"},{"location":"#get-started","title":"Get started","text":"<p>If you do not have a K8s cluster, review the <code>Infrastructure</code> section and spend the time to create a working cluster. Ensure that you can connect to the cluster with kubectl and your identity has admin permission.</p> <p>If you can connect to your K8s cluster with kubectl, assuming the user has admin permission, skip to <code>Deployment</code> section for the next step. </p>"},{"location":"#project-layout","title":"Project Layout","text":"<pre><code>gitops/               # Artifacts for FluxCD to consume\n    application/      # Templates for orthanc workload\n    dependency/       # Templates for dependency services\n    infrastructure/   # Templates for cluster wide services\n    observability/    # Templates for observability addons\n    fluxcd/           # Flux working directory\n        flux-system/  # Flux system directory maintained by Flux controller \n        ...           # Top-level Kustomization definitions\nhelm/\n    orthanc/          # Helm chart directory for Helm approach\nmanual/               # Artifacts for manual approach\ndocs/                 # Documentation pages\nmkdocs.yml            # Documentation configuration\n</code></pre>"},{"location":"deployment/gitops/","title":"GitOps approach with FluxCD","text":"<p>GitOps is the recommended approach and we use FluxCD. This approach deploys two Orthanc tenants, respectively for bhs and mhr (Beaverdale Health Services and Maple Hill Radiology). </p>"},{"location":"deployment/gitops/#architecture","title":"Architecture","text":"<p>At the end of deployment two instances are deployed, in the bhs-orthweb and mhr-orthweb namespaces, as illustrated below:</p> <p></p> <p>Each tenant has its own namespace, where the application, routing and database are hosted. Other services such as cert-manager are shared. Since there are multiple tenants, please ensure that the kubernetes cluster has sufficient capacity. </p>"},{"location":"deployment/gitops/#preparation","title":"Preparation","text":"<p>Ensure that you can connect to your K8s cluster with <code>kubectl</code> from your command terminal. Your user in K8s needs sufficient privilege for Flux to perform deployment activities.</p> <p>Fork this repo to your own GitHub account to serve as the source of deployment. Then obtain your Personal Access Token. Ensure this (classic) token carrier has read-write access to your fork repo. </p> <p>On the command terminal, assign the value of token to environment variable <code>GITHUB_TOKEN</code>: </p> <pre><code>export GITHUB_TOKEN=ghp_yyy55555XXXodr7ABBBB234CCccw\n</code></pre> <p>In the next step, <code>flux</code> will consume this environment variable.</p>"},{"location":"deployment/gitops/#deployment","title":"Deployment","text":"<p>First, we bootstrap the cluster with <code>flux</code>. Suppose the name of your account is digihunch, and the repository name is korthweb, the command to run would be:</p> <pre><code>flux bootstrap github \\\n      --owner=digihunch \\\n      --repository=korthweb \\\n      --branch=main \\\n      --personal \\\n      --path=gitops/fluxcd\n</code></pre> <p>A deployment key will be created. FluxCD will be installed on the cluster, and scans the path specified (gitops/fluxcd) for Kustomization objects.  Kustomization objects defines the sources to sync from. The sync should start automatically (using Kustomization objects) as boostrapping is completed. To check sync progress by kusomization status, run:</p> <pre><code>flux get ks --watch\n</code></pre> <p>It may take 10 minutes to sync all layers. At the end, the output should look like this:</p> <pre><code>NAME            READY   MESSAGE                         REVISION        SUSPENDED\nbhs-application True    Applied revision: main/feffc67  main/feffc67    False\nbhs-dependency  True    Applied revision: main/feffc67  main/feffc67    False\nflux-system     True    Applied revision: main/feffc67  main/feffc67    False\ninfrastructure  True    Applied revision: main/feffc67  main/feffc67    False\nobservability   True    Applied revision: main/feffc67  main/feffc67    False\nmhr-application True    Applied revision: main/feffc67  main/feffc67    False\nmhr-dependency  True    Applied revision: main/feffc67  main/feffc67    False\n</code></pre> <p>When all Kustomizations show True for READY, deployment is completed.</p>"},{"location":"deployment/gitops/#troubleshooting","title":"Troubleshooting","text":"<p>It is important to ensure that the cluster has sufficient capacity, especially if you are running on a single node test cluster. Some stacks will time out when the node does not have sufficient capacity. In case of deployment errors, below are some steps I took to troubleshoot.</p> <ol> <li>Examine status of each Kustomization. The following command is essentially the same as \"fluxctl get ks\":</li> </ol> <pre><code>kubectl -n flux-system get kustomization\n</code></pre> <p>The result may look like this where two kustomizations are waiting for health check results:</p> <pre><code>NAME              READY     STATUS                                                            AGE\nbhs-application   False     dependency 'flux-system/bhs-dependency' is not ready              8m14s\nbhs-dependency    Unknown   running health checks with a timeout of 9m30s                     8m14s\nflux-system       True      Applied revision: main/283db0a641d41637cfc9b6f0f13947d5849e4290   8m47s\ninfrastructure    True      Applied revision: main/283db0a641d41637cfc9b6f0f13947d5849e4290   8m14s\nmhr-application   False     dependency 'flux-system/mhr-dependency' is not ready              8m14s\nmhr-dependency    Unknown   running health checks with a timeout of 9m30s                     8m14s\n</code></pre> <p>This state is normal for a short period of time but if it seems to take forever, we need to inspect the kustomizations pending for health check success.</p> <ol> <li>Review the specific kustomization in question by describing the Kustomization:</li> </ol> <pre><code>kubectl -n flux-system describe kustomization mhr-dependency | less\n</code></pre> <p>The output may include a section like below as Condition:</p> <pre><code>  Conditions:\n    Last Transition Time:  2022-02-04T01:36:26Z\n    Message:               Health check failed after 9m30.012342358s, timeout waiting for: [HelmRelease/mhr-orthweb/postgres-ha status: 'InProgress': context deadline exceeded, Deployment/mhr-orthweb/postgres-ha-postgresql-ha-pgpool status: 'NotFound': context deadline exceeded]\n    Reason:                HealthCheckFailed\n    Status:                False\n    Type:                  Ready\n    Last Transition Time:  2022-02-04T01:36:26Z\n    Message:               HealthCheckFailed\n    Reason:                HealthCheckFailed\n    Status:                False\n    Type:                  Healthy\n</code></pre> <p>The result suggests that Health check for a HelmRelease postgres-ha failed. </p> <ol> <li>Review the status of the specific HelmRelease releated to the failure:</li> </ol> <pre><code>kubectl -n bhs-orthweb describe helmreleases postgres-ha | less\n</code></pre> <p>The result may show the following in Events:</p> <pre><code>Events:\n  Type    Reason  Age   From             Message\n  ----    ------  ----  ----             -------\n  Normal  info    12m   helm-controller  HelmChart 'flux-system/bhs-orthweb-postgres-ha' is not ready\n  Normal  info    11m   helm-controller  Helm install has started\n  Normal  error   11m   helm-controller  Helm install failed: YAML parse error on postgresql-ha/templates/postgresql/statefulset.yaml: error converting YAML to JSON: yaml: line 32: could not find expected ':'\n  ```\n  The error provides the detail of the reason for failure in the Helm Deployment.\n\n4. You can also check the log of Flux:\n```sh\nflux logs\n</code></pre>"},{"location":"deployment/helm/","title":"Helm driven approach","text":"<p>In this approach, we deploy Orthanc with a single helm command, using our self-built Orthanc Helm Chart stored in the orthanc sub-directory. </p>"},{"location":"deployment/helm/#architecture","title":"Architecture","text":"<p>The orthanc Helm Chart automates many activities, including the creation of certificates for the three FQDNs, installation of PostgreSQL using dependency chart, configuring the orthanc workload, and setting up an ingress for HTTP and DICOM traffic. </p> <p>The Helm Chart dependency tree looks like this:</p> <pre><code>                     +--------------+\n                     | Parent chart |\n        +------------|   Orthanc    |-----------+\n        |            --------+------+           |\n        |                                       |\n        v                                       v\n+-------+-------+                       +--------+------+\n|  Sub-chart    |                       |   Sub-chart   |\n| PostgreSQL HA |                       |    Traefik    |\n+---------------+                       +---------------+\n</code></pre> <p>Once the Parent chart has been deployed, the required kubernetes objects (including the ones from the sub-charts) are all deployed and it may take a minute for the Pods to come to READY states. Below is an illustration of Kubernetes objects:</p> <p></p>"},{"location":"deployment/helm/#preparation","title":"Preparation","text":"<p>Instead of publishing in a Helm Repository, the Orthanc Helm chart simply keeps the files in the local sub-directory orthanc. In order to deploy, we need to clone this repo first and enter the helm directory from command terminal:</p> <pre><code>$ git clone git@github.com:digihunch/korthweb.git\n$ cd helm/\n</code></pre> <p>We also need to install Helm client. The Helm client uses the kubectl's connection profile.</p>"},{"location":"deployment/helm/#deployment","title":"Deployment","text":"<p>Since we're in the <code>helm</code> directory, we can update dependency and install the chart:</p> <pre><code>$ helm dependency update orthanc\n$ helm install orthweb orthanc --create-namespace --namespace orthweb \n</code></pre> <p>The installation should be completed once it prints the node. You can monitor the pod status in the orthweb namespace untill all pods are up and running.</p>"},{"location":"deployment/helm/#troubleshooting","title":"Troubleshooting","text":"<p>If you need to uninstall it and remove persistent data, simply run:</p> <pre><code>helm -n orthweb uninstall orthweb &amp;&amp; kubectl -n orthweb delete pvc -l app.kubernetes.io/component=postgresql \n</code></pre> <p>Then the uninstall is done and persistent volumes are removed.</p>"},{"location":"deployment/manual/","title":"Manual approach","text":"<p>This instruction walks through the steps of manual deployment on an estabished Kubernetes cluster</p>"},{"location":"deployment/manual/#architecture","title":"Architecture","text":"<p>The deployment process will install Istio components, observability tools, cert manager, postgres database as well as the Orthanc workload. </p> <p>The ingress will take both web and DICOM traffic on different TCP ports.</p>"},{"location":"deployment/manual/#preparation","title":"Preparation","text":"<p>Ensure that the command terminal has kubectl that connects to the K8s cluster. We also need <code>helm</code> and <code>istoctl</code> to complete manual deployment. </p> <p>Clone the repo and enter the manual directory in command terminal:</p> <pre><code>$ git clone git@github.com:digihunch/korthweb.git\n$ cd helm/\n</code></pre> <p>Most of the artifacts are stored in manual directory. </p>"},{"location":"deployment/manual/#deployment","title":"Deployment","text":"<p>In the manual approach we take a few steps to install Istio and observability add-ons, configure certificates and then deploy Orthanc workload with database.</p>"},{"location":"deployment/manual/#istio-and-observability-add-ons","title":"Istio and observability add-ons","text":"<p>We use <code>istioctl</code> to install istio with the operator manifest in istio directory:</p> <pre><code>istioctl install -f istio/istio-operator.yaml -y --verify\n</code></pre> <p>Then, we install observability addons and view Kiali dashboard (with istioctl or via port-forwarding):</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/addons/jaeger.yaml\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/addons/grafana.yaml\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/addons/prometheus.yaml\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/addons/kiali.yaml\nistioctl dashboard kiali\nkubectl port-forward svc/kiali -n istio-system 8080:20001\n</code></pre> <p>Kiali may take a few minutes to come up. Here we use a single manifest to deploy Kiali just for demo. For full-blown Kiali deployment, we should use Kiali CRD.</p>"},{"location":"deployment/manual/#configure-certificates","title":"Configure Certificates","text":"<p>In this step, we generate our own X.509 key and certificate for the site. The certificates and key are stored as secrets and the Istio Ingress will reference them. To install cert manager using Helm:</p> <pre><code>helm install cert-manager cert-manager --namespace cert-manager --create-namespace --version v1.13.3 --repo https://charts.jetstack.io --set installCRDs=true\n</code></pre> <p>Confrim all Pods in cert-manager namespace come up. Then we use cert-manager CRs to create certificate in istio-system namespace, and verify the certificate by decoding the secret object.</p> <pre><code>kubectl apply -f certs.yaml\n</code></pre>"},{"location":"deployment/manual/#deploy-orthanc-workload","title":"Deploy Orthanc workload","text":"<p>In the <code>orthweb-cm.yaml</code> file, we enable peer authentication and label orthweb namespace as requiring Istio sidecar injection. We also declare the config entry for <code>orthanc.json</code> and database init script. After that, we use Helm to install PostgreSQL database, which will use the init script.</p> <pre><code>kubectl apply -f orthweb-cm.yaml\nhelm install postgres-ha postgresql-ha \\\n       --set postgresql.initdbScriptsCM=dbinit \\\n       --set volumePermissions.enabled=true \\\n       --set service.portName=tcp-postgresql \\\n       --repo https://charts.bitnami.com/bitnami \\\n       --version 12.3.3 \\\n       --namespace orthweb\nkubectl -n orthweb wait deploy/postgres-ha-postgresql-ha-pgpool --for=condition=Available --timeout=10m\nkubectl apply -f orthanc.yaml\nkubectl -n orthweb get po --watch\n</code></pre> <p>As a side note, we store the init script db_create.sql as an entry in orthanc-dbinit config map ahead of time before running the Helm chart, because Helm chart parameter pgpool.initdbScriptsCM does not take file with .sql extension. The postgres pods takes a few mintues to come all up. After that, we deploy the Orthanc workload as declared in <code>orthanc.yaml</code> file.</p>"},{"location":"deployment/overview/","title":"Deployment Overview","text":"<p>While Orthanc application focuses on the core features in medical imaging, to operationalize it on Kubernetes platform, we need numerous auxiliary services for automation, traffic management, observability and security. Luckily, the cloud native ecosystem brings a plethora of open-source choices.</p>"},{"location":"deployment/overview/#source-code-repository","title":"Source Code Repository","text":"<p>The repository for Korthweb is hosted on GitHub at:</p> <p>https://github.com/digihunch/korthweb</p> <p>Please clone the repository to your local command terminal. </p> <pre><code>git clone git@github.com:digihunch/korthweb.git\n</code></pre> <p>For GitOps approach, you will need to fork the korthweb repo to your own GitHub account.</p>"},{"location":"deployment/overview/#deployment-approaches","title":"Deployment Approaches","text":"<p>There are three deployment approaches. Each approach differs in complexity and level of automation but all lead to a functional Orthanc deployment. The approaches are summarized as below:</p> Approach Components Installed Key Features and Considerations GitOps - Istio CRD as Ingress  - Other Service Mesh features supported by Istio  - PostgreSQL  - Cert-Manager - Observability  - Multi-tenancy - Includes artifacts for GitOps-based automated deployment using FluxCD.  - Take this approach for continuous deployment and end-to-end automation.  - Two tenants are deployed, for two fictitious healthcare facilities acronymed BHC and MHR. Helm - Traefik CRD as Ingress  - PostgreSQL - Includes the Helm chart to configure Orthanc and its dependencies with a single command.  - Take this approach to quickly install Orthanc on Kubernetes. Manual - Istio CRD as Ingress  - Other Service Mesh features supported by Istio  - PostgreSQL  - Cert-Manager  - Observability (Lite) - Includes artifacts for users to manually apply.  - Consider this option ONLY for troubleshooting or learning deployment <p>The artifacts of each approach are stored in eponymous sub-directories. Korthweb recommends the GitOps approach.</p>"},{"location":"deployment/overview/#cli-tools","title":"CLI tools","text":"<p>During the deployment process, we need a variety of CLI tools to interact with the cluster, such as:</p> <ul> <li>kubectl: connect to API server to manage the Kubernetes cluster. With multiple clusters, you need to switch context. We need it in all three approaches.</li> <li>helm: helm is package manager for Kubernetes. The name of Helm's CLI tool is <code>helm</code>. We use it in manual and Helm approaches.</li> <li>istioctl: in the manual approach we use <code>istioctl</code> to install Istio.</li> <li>flux: FluxCD is a GitOps tool to keep target Kubernetes cluster in sync with the source of configuration in the GitOps directory. The name of FluxCD's CLI tool is <code>flux</code>, and we use it in the GitOps approach.</li> </ul> <p>Have them installed on your local environment. Ensure that <code>kubectl</code> connects to the cluster correctly. Other CLI tools <code>helm</code>, <code>istioctl</code> and <code>flux</code> all use <code>kubectl</code>'s connection profile.</p>"},{"location":"deployment/overview/#orthanc-configuration-file","title":"Orthanc Configuration File","text":"<p>When running as an Operating System process (non-containerized runtime), the Orthanc process expects a JSON file as  configuration input. When we host Orthanc application in containers on Kubernetes, we run multiple Orthanc Pods. We have to make sure all the Orthanc Pods share a single instance of the configuration file. </p> <p>Kubernetes ConfigMaps object fulfills our target here. We store the content of the JSON file as a ConfigMap. The name of the ConfigMap is <code>orthanc-app</code>. When we launch an Orthanc Pod, we tell the Pod to mount the ConfigMap entry as a file within the Pod for the Orthanc process in the Pod to consume.</p>"},{"location":"infrastructure/","title":"Kubernetes Cluster","text":"<p>Building a production grade Kubernetes cluster is a professional effort that is beyond the scope of this project. Skip this section if you already have a sandbox cluster. Otherwise, </p>"},{"location":"infrastructure/#sandbox-cluster","title":"Sandbox cluster","text":"<p>The CloudKube project provides Terraform templates to create Kubernetes cluster along with networking layer in Azure and AWS. These clusters represents what a production cluster looks like in real life and it takes about 30 minutes to deploy. </p> <p>For quick evalucation of Kubernetes workload, developers builds their own sandbox Kubernetes cluster running on MacBook or PC. There are many ways to build a sandbox cluster. </p> <p>I do not recommend the Kubernetes distro that came with Docker desktop. It is a single node and as of aug 2021 it does not use containerd as CRI. Read this post about the details.</p> <p>Refer to the real-quicK-cluster project for more about how to set up a sandbox cluster depending on your operating system. Refer to this post for the differences between options.</p>"},{"location":"infrastructure/#storage","title":"Storage","text":"<p>Kubernetes platform hosts applications but it does not by itself provide storage. It needs to integrate with external storage services via Container Storage Interface. </p> <p>The Osimis image includes Orthanc S3 plugin, which allows to store images as objects. Alternatively, you can store images on file systems mounted to Pods.</p> <p>With S3 plugin, you can either use Amazon S3, or any self-hosted S3-compatible object storage. MinIO is a good choice. The configuration is not covered in this guide.</p> <p>For file storage, you will need to configure storage classes and persistent volumes. In each cloud platform, there is some pre-built storage classes to consider. For example, this post discusses the storage options on Azure Kubernetes.</p> <p>Another option is to consider an SDS (software defined storage) layer. For SDS-based storage solution, you may use a proprietary solution such as Portworx, or an open-source self-hosted alternative such as Ceph by Rook. Here is a post that discusses the setup on Azure Kubernetes.</p>"},{"location":"infrastructure/#database","title":"Database","text":"<p>In Korthweb, Orthanc connects to PostgreSQL database. Korthweb hosts database on Kubernetes. Database is a stateful workload and maintaining it on Kubernetes is involving. For that sake, many favour the alternative hosting models for database: using a managed database service from cloud provider. Refer to blog post Hosting database on Kubernetes for more pros and cons of this topic.</p> <p>In general, if your team does not have the resource to administer the database (e.g. configure replication, expanding, configure storage, backup, patching, updates, etc), you should consider managed database service by cloud provider, such as Azure database for PostgreSQL, Amazon RDS/Aurora PostgreSQL or GCP Cloud SQL for PostgreSQL. </p>"},{"location":"infrastructure/#limitation","title":"Limitation","text":"<p>The context provided above is to raise awareness so that the complextity of system architecture is not underestimated. Being a demo project, the Korthweb deployment has taken some \"happy paths\" and this section explains what the simplifcations are.</p> <p>In terms of database, Korthweb configures Orthanc to store images in PostgreSQL database by setting EnableStorage to true under PostgreSQL plugin configuration. In production, it is better to store images separately using object storage or persistent volumes. </p>"},{"location":"security/","title":"Security of Korthweb solution","text":"<p>While Korthweb is a demo project, we take security seriously. We look at security from the perspecives of the network infrastructure, cluster and the workload.</p>"},{"location":"security/#network-infrastructure-security","title":"Network infrastructure security","text":"<p>Network infrastructure design is beyond the scope of this project. It is usually the networking or cloud engineering team of an organization to ensure the security and compliance of their networking foundation.</p>"},{"location":"security/#cluster-security","title":"Cluster Security","text":"<p>As discussed, K8s cluster design is also beyond the scope of this project. It is usually the platform team to ensure the security configuration of Kubernetes cluster. Refer to OWASP Cheatsheet, and CIS benchmark for more details.</p>"},{"location":"security/#workload-security","title":"Workload Security","text":"<p>Korthweb deployment follows best practices to ensure workload security. Take the GitOps apporach as an example:</p> <ol> <li>Both DICOM and web traffic are encrypted in TLS. The deployment process creates self-signed certificate. The GitOps and manual approaches use Cert Manager. The Helm approach use Helm's built-in cryptographic functions.</li> <li>Connections between Orthanc Pods and PostgreSQL are encrypted with mTLS, provided by Istio service mesh.</li> <li>Istio's Peer Authentication applies mTLS for any service-to-service traffic. Refer to the architecture section for how Istio can enhance the security setup.</li> <li>The workload for the two tenants are seperated logically with their own namespaces.</li> </ol>"},{"location":"support/","title":"Support","text":"<p>Korthweb is a demo project that focues on deployment automation and security. For feature request or bugs, please open an Issue.</p> <p>For further support, contact Digi Hunch for professional service. Some areas of support include:</p> <ul> <li>Orthanc Deployment in customized environment</li> <li>Integration with your environment and platform</li> <li>Build secure and compliant Kubernetes platform</li> <li>Build secure landing zone for your cloud footprint</li> </ul> <p>Thanks for testing with Korthweb.</p>"},{"location":"tooling/","title":"Platform Toolings","text":"<p>This page discusses the choice of tools used in Korthweb deployment. Below is a summary of the technologies adopted in the Korthweb deployment process.</p>"},{"location":"tooling/#ingress","title":"Ingress","text":"<p>At container level, Orthanc uses TCP port 8042 for web traffic, and TCP port 4242 for DICOM traffic. On Kubernetes, we use ingress to expose both ports (443 for web and 11112 for DICOM). The ingress controller also does TLS termination and load balancing.</p> <p>The Helm chart driven option uses Traefik CRD for Ingress. The GitOps driven and manual approaches uses Istio Ingress CRD. For more details, read my post on how to choose the right ingress technology.  </p>"},{"location":"tooling/#istio","title":"Istio","text":"<p>Service mesh acts as an intermediary layer between the application workload and the underlying platform. This layer commoditizes a variety of common features, such as tracing, mTLS, traffic routing and management. While an application may choose build these features natively in its own code, the idea of service mesh is to allow application developer to focus on the business logic and push networking concerns to this intermediary layer.</p> <p>Istio is a service mesh product. We use it for Ingress, TLS termination, mTLS, authorization and observability. Once deployed, service-to-service connections (e.g. application to database) automatically take place in mTLS and there is no need to explicitly configure TLS on database connection, as is done in the Helm Chart approach.</p> <p>We need to control ingress traffic from client into the K8s cluster with an Ingress Controller, e.g. request routing, TLS termination Korthweb uses Traefik CRD, or Istio Ingress CRD for this requirement.</p>"},{"location":"tooling/#observability","title":"Observability","text":"<p>Applications today are released as container images and are hosted as microservices, which brings new challenges to observability.</p> <p>Istio supports observability add-ons, such as Prometheus to expose envoy (Istio sidecar) metrics and Grafana and Kiali for dashboard display. In manual or GitOps approaches, Kiali is neither exposed on Ingress gateway, or integrated with any IAM system. To access Kiali, we can use port-forwarding. Refer to the instruction in each approach.</p> <p>We need to measure responsiveness of requests and trace requests to Orthanc. Korthweb utilizes observability addons such as Prometheus and Grafana</p>"},{"location":"tooling/#helm","title":"Helm","text":"<p>We play with Helm in two ways: reusing other's Chart and building our own. Bitnami publishes Helm Charts for common applications (e.g. PostgreSQL) and we simply piggyback on their great work wherever applicable, by deploying their Charts in our platform.</p> <p>In addition, with the Helm Chart approach, we also build our own Helm Chart (named orthanc) to package Orthanc workload along with dependencies. Our Helm chart makes Orthanc deployment a single command. We did not include Istio in this approach for simplicity. Instead of Istio as Gateway, we use Traefik CRD for Ingress.</p>"},{"location":"tooling/#fluxcd","title":"FluxCD","text":"<p>FluxCD is a tool to drive GitOps-based deployment. GitOps is a relatively new deployment approach. With GitOps, source of truth about the deployment is declared in the GitOps directory of this repository or your fork. FluxCD is installed in the target cluster, which watches the source and keeps the target kubernetes cluster state in sync. The GitOps directory serves as the source for deployment. Read the instruction in the directory for more details.</p> <p>The GitOps approach is more flexible than the Helm approach and can accomodate many deployment techniques. However it is more complicated to use than the Helm approach.</p>"},{"location":"tooling/#dicom-testing","title":"DICOM Testing","text":"<p>Regardless of deployment option, users need to validate DICOM capability. Each option provides dcmtk commands running C-ECHO and C-STORE against their respective DICOM endpoints. All DICOM communication are TLS enabled and correct testing involves understanding of how TLS works. Read my blog post on DICOM testing guidelines. </p> <p>In addition, the automated deployment needs provision self-signed certificates and reference them accordingly. Korthweb uses Cert Manager to meet this requirement.</p> <p>Korthweb provides artifacts to automatically deploy all the foundational services as discussed above. </p>"},{"location":"validation/gitops/","title":"Validation for GitOps approach","text":""},{"location":"validation/gitops/#preparation","title":"Preparation","text":"<p>The GitOps approach deploys two tenants: MHR and BHS. There are two namespaces mhr-orthweb and bhs-orthweb, both of which should be tested.</p> <p>The first step is to make sure DNS resolution works. If you're on a Sandbox cluster such as Minikube, you may have to mock DNS resolution to ingress IP by adding the following entries to <code>/etc/hosts</code>:</p> <pre><code>192.168.64.16 web.bhs.orthweb.com\n192.168.64.16 dicom.bhs.orthweb.com\n192.168.64.17 web.mhr.orthweb.com\n192.168.64.17 dicom.mhr.orthweb.com\n</code></pre> <p>To find out the IP address, look for the attribute in each ingress service. For exampe:</p> <pre><code>kubectl -n bhs-orthweb get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre>"},{"location":"validation/gitops/#generate-client-certificate","title":"Generate client certificate","text":"<p>Take BHS tenant as an example, we first create a client certificate for use later.</p> <pre><code># bhs: generate client key pair\nopenssl req -new -newkey rsa:4096 -nodes -subj /C=CA/ST=Ontario/L=Waterloo/O=Digihunch/OU=Imaging/CN=dcmclient.bhs.orthweb.com/emailAddress=dcmclient@digihunch.com -keyout bhs.client.key -out bhs.client.csr\n\n# bhs: export intermediate CA credentials\nkubectl -n bhs-orthweb get secret int-ca-secret -o jsonpath='{.data.tls\\.key}' | base64 -d &gt; bhs.int.ca.key\nkubectl -n bhs-orthweb get secret int-ca-secret -o jsonpath='{.data.tls\\.crt}' | base64 -d &gt; bhs.int.ca.crt\n\n# bhs: get intermediate CA to sign client cert \nopenssl x509 -req -sha256 -days 365 -in bhs.client.csr -CA bhs.int.ca.crt -CAkey bhs.int.ca.key -set_serial 01 -out bhs.client.crt\n</code></pre>"},{"location":"validation/gitops/#validate-web-service","title":"Validate web service","text":"<p>Take BHS tenant as an example, we can test web service with curl command as below:</p> <pre><code># bhs: validate web request (without client certificate)\ncurl -HHost:web.bhs.orthweb.com -k -X GET https://web.bhs.orthweb.com:443/app/explorer.html -u admin:orthanc --cacert bhs.int.ca.crt\n</code></pre> <p>Alternatively we can browse to the URL. However, the browser may flag the self-signed certificate as insecure.</p>"},{"location":"validation/gitops/#validate-dicom-service","title":"Validate DICOM service","text":"<p>Take BHS tenant as an example, we can test DICOM C-ECHO and C-STORE with the following commands:</p> <pre><code># bhs: validate DICOM c-echo request (with client certificate)\nechoscu -aet TESTER -aec ORTHANC -d +tls bhs.client.key bhs.client.crt -rc +cf bhs.int.ca.crt dicom.bhs.orthweb.com 11112\n\n# bhs: validate DICOM c-store request (with client certificate)\nstorescu -aet TESTER -aec ORTHANC -d +tls bhs.client.key bhs.client.crt -rc +cf bhs.int.ca.crt dicom.bhs.orthweb.com 11112 DICOM_CT/0001.dcm\n</code></pre>"},{"location":"validation/gitops/#verify-observability","title":"Verify Observability","text":"<p>To check Pod logs, use Kiali. We can use port-forward to expose kiali service.</p> <pre><code>kubectl port-forward svc/kiali -n monitoring 8080:20001\n</code></pre>"},{"location":"validation/helm/","title":"Validation for Helm approach","text":""},{"location":"validation/helm/#preparation","title":"Preparation","text":"<p>First, find out the external IP address for traefik ingress:</p> <pre><code>kubectl -n orthweb get service orthweb-traefik -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre> <p>Ensure that the DNS names web.orthweb.com and dicom.orthweb.com resolve to the external IP address of the traefik service. </p>"},{"location":"validation/helm/#web-service","title":"Web Service","text":"<p>To validate web service, export client CA and run curl command:</p> <pre><code>kubectl -n orthweb get secret https-secret -o jsonpath='{.data.ca\\.crt}' | base64 -d &gt; ca.crt\n\ncurl -HHost:web.orthweb.com -v -k -X GET https://web.orthweb.com:443/app/explorer.html -u admin:orthanc --cacert ca.crt\n</code></pre> <p>You should see HTML content of the website. Alternatively browse to the URL. However, browser may flag the self-signed certificate as insecure.</p>"},{"location":"validation/helm/#dicom-service","title":"DICOM service","text":"<p>The steps to validate DICOM traffic is similiar to other deployment option. However, because dcmtk utility does not send SNI in the TLS negotiation, I used annonymous tls (+tla) without client certificate for C-ECHO and C-STORE test.</p> <pre><code>echoscu -aet TESTER -aec ORTHANC -d +tla -ic dicom.orthweb.com 11112\nstorescu -aet TESTER -aec ORTHANC -d +tla -ic dicom.orthweb.com 11112 DICOM_CT/123.dcm\n</code></pre>"},{"location":"validation/manual/","title":"Validation for manual approach","text":""},{"location":"validation/manual/#preparation","title":"Preparation","text":"<p>First, find out the IP address of istio ingress service:</p> <pre><code>kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre> <p>On this IP address, we expose port 443 for HTTPS traffic and 11112 for DICOM traffic over TLS. It is important to ensure that both web endpoint (web.orthweb.com) and DICOM endpoint(dicom.orthweb.com) resolve to this IP address. If you do not have control over DNS resolution, consider adding the IP address to host file (e.g. /etc/hosts) on your testing machine, for example:</p> <pre><code>192.168.64.16 web.orthweb.com\n192.168.64.16 dicom.orthweb.com\n</code></pre>"},{"location":"validation/manual/#web-service","title":"Web service","text":"<p>We can quickly test the web endpiont by browsing to https://web.orthweb.com:443/app/explorer.html or use curl command:</p> <pre><code>kubectl -n istio-system get secret ca-secret -o jsonpath='{.data.tls\\.crt}' | base64 -d &gt; ca.crt\n\ncurl -HHost:web.orthweb.com -k -X GET https://web.orthweb.com:443/app/explorer.html -u admin:orthanc --cacert ca.crt\n\n</code></pre> <p>Note that we take a step to export CA's certificate so we can tell curl to trust the server's certificate issued by the CA.</p> <p>To examine whether the server certificate is configured correctly on an endpoint, we can use openssl command:</p> <pre><code>openssl s_client -showcerts -connect web.orthweb.com:443 -servername web.orthweb.com &lt; /dev/null\n\nopenssl s_client -showcerts -connect dicom.orthweb.com:11112 -servername dicom.orthweb.com &lt; /dev/null\n</code></pre>"},{"location":"validation/manual/#create-client-certificate","title":"Create client certificate","text":"<p>To validate DICOM endpoint, the client should carry its own certificate. To achieve that, we need to generate a key pair for the client. Then we need to export the CA's key so we can get the CA to sign the certificate of the client</p> <pre><code>openssl req -new -newkey rsa:4096 -nodes -subj /C=CA/ST=Ontario/L=Waterloo/O=Digihunch/OU=Imaging/CN=dcmclient.orthweb.digihunch.com/emailAddress=dcmclient@digihunch.com -keyout client.key -out client.csr\n\nkubectl -n istio-system get secret ca-secret -o jsonpath='{.data.tls\\.key}' | base64 -d &gt; ca.key\n\nopenssl x509 -req -sha256 -days 365 -in client.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out client.crt\n</code></pre>"},{"location":"validation/manual/#dicom-service","title":"DICOM service","text":"<p>We can provide the <code>client.key</code> and <code>client.crt</code> to dcmtk executables. Take C-ECHO as an example, we can use the following command:</p> <pre><code>echoscu -aet TESTER -aec ORTHANC -d +tls client.key client.crt -rc +cf ca.crt dicom.orthweb.com 11112\n</code></pre> <p>The result should report success. To C-STORE an image, we can run:</p> <pre><code>storescu -aet TESTER -aec ORTHANC -d +tls client.key client.crt -rc +cf ca.crt dicom.orthweb.com 11112 DICOM_CT/COVID/56364504.dcm\n</code></pre> <p>The result should report success with C-STORE with return code 0. From the web portal you should be able to see the image sent.</p>"},{"location":"validation/manual/#database-connectivity","title":"Database connectivity","text":"<p>Sometimes one needs to validate connectivity from orthweb namespace to Postgres database. To do so, connect to Bash terminal of a sleeper pod (or a workload pod with some args commented out), and run:</p> <pre><code>$ export PGPASSWORD=$DB_PASSWORD &amp;&amp; apt update &amp;&amp; apt install postgresql postgresql-contrib\n$ psql --host=$DB_ADDR --port $DB_PORT --username=$DB_USERNAME sslmode=require\n</code></pre> <p>From there you should be able to execute SQL statements.</p>"},{"location":"validation/overview/","title":"Validation","text":"<p>This validation guide is not intended to be an exhaustive list of check points. However, we check both TCP ports service clients with one service. </p>"},{"location":"validation/overview/#web-traffic","title":"Web traffic","text":"<p>The web service is hosted on port 443. User will need to browse to the URL with a Browser or use <code>curl</code> command instead. The traffic is HTTPS by default with a self-signed certificate configured during the deployment process. In addition, users should supply the default username and password (admin:orthanc) when logging on to the web portal. </p>"},{"location":"validation/overview/#dicom-traffic","title":"DICOM traffic","text":"<p>The DICOM service is hosted on TCP port 11112. User can verify the port by issuing a C-ECHO DIMSE command to this port, and then send images to the port using C-STORE DIMSE command. DICOM traffic is protected by TLS by default.</p>"},{"location":"validation/overview/#utilities","title":"Utilities","text":"<p>The validation steps involves the following tools:</p> <ul> <li>We still need <code>kubectl</code> to examine object in Kubernetes, such as exporting CA information;</li> <li>The <code>curl</code> tool as HTTP client;</li> <li><code>openssl</code> to create client certificate under the same CA with the server;</li> <li><code>dcmtk</code> as DICOM client. The dcmtk package includes many executables such as <code>echoscu</code> and <code>storescu</code>. </li> </ul> <p>For more details about DICOM testing over TLS, such as the limitation with SNI, Refer to this post. </p>"}]}